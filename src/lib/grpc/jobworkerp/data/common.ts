// Code generated by protoc-gen-ts_proto. DO NOT EDIT.
// versions:
//   protoc-gen-ts_proto  v2.8.3
//   protoc               v6.33.1
// source: jobworkerp/data/common.proto

/* eslint-disable */
import { BinaryReader, BinaryWriter } from "@bufbuild/protobuf/wire";

export const protobufPackage = "jobworkerp.data";

/**
 * # Storage Type
 * Defines the storage backend type used for job queue and result storage
 */
export enum StorageType {
  /**
   * STANDALONE - # Standalone Mode
   * Uses memory channel and SQLite for job queue and result storage
   * Suitable for single-node deployments
   */
  STANDALONE = 0,
  /**
   * SCALABLE - # Scalable Mode
   * Uses Redis and MySQL for job queue and result storage
   * Suitable for distributed deployments
   */
  SCALABLE = 1,
  UNRECOGNIZED = -1,
}

/**
 * # Runner Type
 * Defines the built-in runner implementation types
 */
export enum RunnerType {
  /**
   * PLUGIN - # Plugin Runner
   * Executes jobs through custom plugin modules
   */
  PLUGIN = 0,
  /**
   * COMMAND - # Command Runner
   * Executes shell commands
   */
  COMMAND = 1,
  /**
   * HTTP_REQUEST - # HTTP Request Runner
   * Makes HTTP requests to specified endpoints
   */
  HTTP_REQUEST = 2,
  /**
   * GRPC_UNARY - # gRPC Unary Runner
   * Makes unary gRPC calls to specified services
   */
  GRPC_UNARY = 3,
  /**
   * DOCKER - # Docker Runner
   * Executes commands inside Docker containers
   */
  DOCKER = 4,
  /**
   * SLACK_POST_MESSAGE - # Slack Message Runner
   * Posts messages to Slack channels
   */
  SLACK_POST_MESSAGE = 5,
  /**
   * PYTHON_COMMAND - # Python Command Runner
   * Executes Python scripts as commands
   */
  PYTHON_COMMAND = 6,
  /**
   * MCP_SERVER - # Mcp Server Runner
   * Executes tools of MCP servers
   */
  MCP_SERVER = 7,
  /**
   * LLM_CHAT - # LLM Chat Completion API Runner
   * Interacts with Large Language Models
   */
  LLM_CHAT = 65533,
  /**
   * LLM_COMPLETION - # LLM Completion API Runner
   * Interacts with Large Language Models
   */
  LLM_COMPLETION = 65534,
  /**
   * INLINE_WORKFLOW - # Inline Workflow Runner
   * Executes a sequence of jobs as a workflow defined inline with the request
   * Jobs are executed in order and not preserved after execution completes
   */
  INLINE_WORKFLOW = 65535,
  /**
   * REUSABLE_WORKFLOW - # Reusable Workflow Runner
   * Allows users to define, save, and reuse workflow definitions across
   * multiple executions Workflows are stored in the system and can be executed
   * by reference
   */
  REUSABLE_WORKFLOW = 65532,
  /**
   * CREATE_WORKFLOW - # Create Workflow Runner
   * Creates workflow definitions that can be stored and reused
   * This runner is specifically designed for workflow creation, separate from execution
   */
  CREATE_WORKFLOW = -1,
}

/**
 * # Priority Level
 * Defines the execution priority of a job
 */
export enum Priority {
  /**
   * PRIORITY_MEDIUM - # Medium Priority (Default)
   * Standard priority level for most jobs
   */
  PRIORITY_MEDIUM = 0,
  /**
   * PRIORITY_LOW - # Low Priority
   * Lower priority, executed after medium priority jobs
   */
  PRIORITY_LOW = -1,
  /**
   * PRIORITY_HIGH - # High Priority
   * Higher priority, executed before medium priority jobs
   */
  PRIORITY_HIGH = 1,
}

/**
 * # Retry Type
 * Defines the algorithm used for retry intervals
 */
export enum RetryType {
  /**
   * NONE - # No Retry
   * Failed jobs will not be retried
   */
  NONE = 0,
  /**
   * EXPONENTIAL - # Exponential Backoff
   * Retry interval increases exponentially with each attempt
   * Formula: interval * basis ^ (retry_count - 1)
   */
  EXPONENTIAL = 1,
  /**
   * LINEAR - # Linear Backoff
   * Retry interval increases linearly with each attempt
   * Formula: interval * retry_count
   */
  LINEAR = 2,
  /**
   * CONSTANT - # Constant Interval
   * Same retry interval is used for all retry attempts
   */
  CONSTANT = 3,
  UNRECOGNIZED = -1,
}

/**
 * # Response Type
 * Defines how job results should be returned to the client
 */
export enum ResponseType {
  /**
   * NO_RESULT - # No Result
   * Returns job ID immediately, but no result
   * Results may be available in database if stored by worker settings
   * If broadcasting is enabled (worker.broadcast_result=true),
   * the result will be broadcasted to all subscribers (use
   * JobResultService/Listen)
   */
  NO_RESULT = 0,
  /**
   * DIRECT - # Direct Response
   * Gets result in the same response
   * May involve long wait (long polling with gRPC unary)
   */
  DIRECT = 1,
  UNRECOGNIZED = -1,
}

/**
 * # Result Status
 * Defines the execution outcome of a job
 */
export enum ResultStatus {
  /**
   * SUCCESS - # Success
   * Job completed successfully
   */
  SUCCESS = 0,
  /**
   * ERROR_AND_RETRY - # Error with Retry
   * Job failed but will be retried
   */
  ERROR_AND_RETRY = 1,
  /**
   * FATAL_ERROR - # Fatal Error
   * Job failed with a fatal error (cannot retry)
   */
  FATAL_ERROR = 2,
  /**
   * ABORT - # Aborted
   * Job was manually aborted (cannot retry)
   */
  ABORT = 3,
  /**
   * MAX_RETRY - # Maximum Retries Reached
   * Job failed and reached maximum retry count (cannot retry further)
   */
  MAX_RETRY = 4,
  /**
   * OTHER_ERROR - # Other Error
   * Unexpected system error occurred (cannot retry)
   */
  OTHER_ERROR = 5,
  /**
   * CANCELLED - # Cancelled
   * Job was cancelled by user request (cannot retry)
   */
  CANCELLED = 6,
  UNRECOGNIZED = -1,
}

/**
 * # Job Processing Status
 * Defines the current processing state of a job (temporary state during execution)
 * This is different from ResultStatus which represents permanent job outcomes
 */
export enum JobProcessingStatus {
  /**
   * UNKNOWN - # Unknown
   * Job status is unknown or corrupted (default value)
   */
  UNKNOWN = 0,
  /**
   * PENDING - # Pending
   * Job is waiting to be processed
   */
  PENDING = 1,
  /**
   * RUNNING - # Running
   * Job is currently being executed
   */
  RUNNING = 2,
  /**
   * WAIT_RESULT - # Waiting for Result
   * Job has completed execution and is waiting for result processing
   */
  WAIT_RESULT = 3,
  /**
   * CANCELLING - # Cancelling
   * Job is being cancelled (temporary state before final cancellation)
   */
  CANCELLING = 4,
  UNRECOGNIZED = -1,
}

/**
 * # Queue Type
 * Defines how jobs are queued and persisted
 */
export enum QueueType {
  /**
   * NORMAL - # Normal Queue
   * Standard in-memory queue with no persistence
   */
  NORMAL = 0,
  /**
   * WITH_BACKUP - # Queue with Backup
   * In-memory queue with database backup for recovery (slower)
   */
  WITH_BACKUP = 1,
  /**
   * DB_ONLY - # Database-Only Queue
   * Forces all jobs to be stored in database (slowest)
   */
  DB_ONLY = 2,
  UNRECOGNIZED = -1,
}

/**
 * # Streaming Type
 * Defines how streaming is handled for job execution
 * Note: Values are chosen for backward compatibility with request_streaming bool field:
 *   - 0 (false) = NONE, 1 (true) = RESPONSE
 */
export enum StreamingType {
  /**
   * STREAMING_TYPE_NONE - # No Streaming
   * Job execution is non-streaming. Runner's run() method is used.
   * This is the default behavior.
   * Compatible with request_streaming=false
   */
  STREAMING_TYPE_NONE = 0,
  /**
   * STREAMING_TYPE_RESPONSE - # Response Streaming
   * Full streaming mode. Runner uses run_stream() and results are
   * streamed back to the client via pub/sub.
   * Client receives ResultOutputItem stream.
   * Compatible with request_streaming=true
   */
  STREAMING_TYPE_RESPONSE = 1,
  /**
   * STREAMING_TYPE_INTERNAL - # Internal Streaming
   * Runner internally uses streaming (run_stream()) but the result is
   * collected using collect_stream() and returned as a single JobResult.
   * Useful for workflow tasks that need the final result but want to use
   * streaming-capable runners internally for better resource management.
   */
  STREAMING_TYPE_INTERNAL = 2,
  UNRECOGNIZED = -1,
}

/**
 * # Empty Message
 * Represents an empty message with no fields, used as a placeholder
 */
export interface Empty {
}

/** # Trailer */
export interface Trailer {
  /**
   * # metadata
   * Additional metadata information
   */
  metadata: { [key: string]: string };
}

export interface Trailer_MetadataEntry {
  key: string;
  value: string;
}

/**
 * # Retry Policy
 * Configuration for how failed jobs should be retried
 */
export interface RetryPolicy {
  /**
   * # Retry Type
   * Algorithm used for calculating retry intervals
   */
  type: RetryType;
  /**
   * # Base Interval
   * Basic retry interval in milliseconds
   */
  interval: number;
  /**
   * # Maximum Interval
   * Maximum retry interval in milliseconds
   */
  maxInterval: number;
  /**
   * # Maximum Retry Count
   * Maximum number of retry attempts (0: unlimited)
   */
  maxRetry: number;
  /**
   * # Exponential Basis
   * Base for exponential backoff calculation
   * Formula: interval * basis ^ (retry_count - 1)
   */
  basis: number;
}

/**
 * # Result Output Item
 * Individual item in a streaming result output
 */
export interface ResultOutputItem {
  /**
   * # Data Chunk
   * A piece of result data in a stream
   */
  data?:
    | Uint8Array
    | undefined;
  /**
   * # End of Stream
   * Marker indicating the end of the stream
   */
  end?:
    | Trailer
    | undefined;
  /**
   * # Final Collected Result
   * Contains the final collected result for STREAMING_TYPE_INTERNAL mode.
   * This is sent as the last item in the stream after all data chunks,
   * containing the aggregated result from collect_stream().
   * Workflow consumers should use this as the task output.
   */
  finalCollected?: Uint8Array | undefined;
}

/**
 * # Result Output
 * Collection of result data items for non-streaming output
 */
export interface ResultOutput {
  /**
   * # Result Items
   * List of binary data chunks representing the result
   */
  items: Uint8Array;
}

function createBaseEmpty(): Empty {
  return {};
}

export const Empty: MessageFns<Empty> = {
  encode(_: Empty, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Empty {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseEmpty();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Empty>): Empty {
    return Empty.fromPartial(base ?? {});
  },
  fromPartial(_: DeepPartial<Empty>): Empty {
    const message = createBaseEmpty();
    return message;
  },
};

function createBaseTrailer(): Trailer {
  return { metadata: {} };
}

export const Trailer: MessageFns<Trailer> = {
  encode(message: Trailer, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    Object.entries(message.metadata).forEach(([key, value]) => {
      Trailer_MetadataEntry.encode({ key: key as any, value }, writer.uint32(10).fork()).join();
    });
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Trailer {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrailer();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          const entry1 = Trailer_MetadataEntry.decode(reader, reader.uint32());
          if (entry1.value !== undefined) {
            message.metadata[entry1.key] = entry1.value;
          }
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Trailer>): Trailer {
    return Trailer.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Trailer>): Trailer {
    const message = createBaseTrailer();
    message.metadata = Object.entries(object.metadata ?? {}).reduce<{ [key: string]: string }>((acc, [key, value]) => {
      if (value !== undefined) {
        acc[key] = globalThis.String(value);
      }
      return acc;
    }, {});
    return message;
  },
};

function createBaseTrailer_MetadataEntry(): Trailer_MetadataEntry {
  return { key: "", value: "" };
}

export const Trailer_MetadataEntry: MessageFns<Trailer_MetadataEntry> = {
  encode(message: Trailer_MetadataEntry, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.key !== "") {
      writer.uint32(10).string(message.key);
    }
    if (message.value !== "") {
      writer.uint32(18).string(message.value);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): Trailer_MetadataEntry {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseTrailer_MetadataEntry();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.key = reader.string();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.value = reader.string();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<Trailer_MetadataEntry>): Trailer_MetadataEntry {
    return Trailer_MetadataEntry.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<Trailer_MetadataEntry>): Trailer_MetadataEntry {
    const message = createBaseTrailer_MetadataEntry();
    message.key = object.key ?? "";
    message.value = object.value ?? "";
    return message;
  },
};

function createBaseRetryPolicy(): RetryPolicy {
  return { type: 0, interval: 0, maxInterval: 0, maxRetry: 0, basis: 0 };
}

export const RetryPolicy: MessageFns<RetryPolicy> = {
  encode(message: RetryPolicy, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.type !== 0) {
      writer.uint32(8).int32(message.type);
    }
    if (message.interval !== 0) {
      writer.uint32(16).uint32(message.interval);
    }
    if (message.maxInterval !== 0) {
      writer.uint32(24).uint32(message.maxInterval);
    }
    if (message.maxRetry !== 0) {
      writer.uint32(32).uint32(message.maxRetry);
    }
    if (message.basis !== 0) {
      writer.uint32(45).float(message.basis);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): RetryPolicy {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseRetryPolicy();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 8) {
            break;
          }

          message.type = reader.int32() as any;
          continue;
        }
        case 2: {
          if (tag !== 16) {
            break;
          }

          message.interval = reader.uint32();
          continue;
        }
        case 3: {
          if (tag !== 24) {
            break;
          }

          message.maxInterval = reader.uint32();
          continue;
        }
        case 4: {
          if (tag !== 32) {
            break;
          }

          message.maxRetry = reader.uint32();
          continue;
        }
        case 5: {
          if (tag !== 45) {
            break;
          }

          message.basis = reader.float();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<RetryPolicy>): RetryPolicy {
    return RetryPolicy.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<RetryPolicy>): RetryPolicy {
    const message = createBaseRetryPolicy();
    message.type = object.type ?? 0;
    message.interval = object.interval ?? 0;
    message.maxInterval = object.maxInterval ?? 0;
    message.maxRetry = object.maxRetry ?? 0;
    message.basis = object.basis ?? 0;
    return message;
  },
};

function createBaseResultOutputItem(): ResultOutputItem {
  return { data: undefined, end: undefined, finalCollected: undefined };
}

export const ResultOutputItem: MessageFns<ResultOutputItem> = {
  encode(message: ResultOutputItem, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.data !== undefined) {
      writer.uint32(10).bytes(message.data);
    }
    if (message.end !== undefined) {
      Trailer.encode(message.end, writer.uint32(18).fork()).join();
    }
    if (message.finalCollected !== undefined) {
      writer.uint32(26).bytes(message.finalCollected);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultOutputItem {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultOutputItem();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.data = reader.bytes();
          continue;
        }
        case 2: {
          if (tag !== 18) {
            break;
          }

          message.end = Trailer.decode(reader, reader.uint32());
          continue;
        }
        case 3: {
          if (tag !== 26) {
            break;
          }

          message.finalCollected = reader.bytes();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ResultOutputItem>): ResultOutputItem {
    return ResultOutputItem.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultOutputItem>): ResultOutputItem {
    const message = createBaseResultOutputItem();
    message.data = object.data ?? undefined;
    message.end = (object.end !== undefined && object.end !== null) ? Trailer.fromPartial(object.end) : undefined;
    message.finalCollected = object.finalCollected ?? undefined;
    return message;
  },
};

function createBaseResultOutput(): ResultOutput {
  return { items: new Uint8Array(0) };
}

export const ResultOutput: MessageFns<ResultOutput> = {
  encode(message: ResultOutput, writer: BinaryWriter = new BinaryWriter()): BinaryWriter {
    if (message.items.length !== 0) {
      writer.uint32(10).bytes(message.items);
    }
    return writer;
  },

  decode(input: BinaryReader | Uint8Array, length?: number): ResultOutput {
    const reader = input instanceof BinaryReader ? input : new BinaryReader(input);
    const end = length === undefined ? reader.len : reader.pos + length;
    const message = createBaseResultOutput();
    while (reader.pos < end) {
      const tag = reader.uint32();
      switch (tag >>> 3) {
        case 1: {
          if (tag !== 10) {
            break;
          }

          message.items = reader.bytes();
          continue;
        }
      }
      if ((tag & 7) === 4 || tag === 0) {
        break;
      }
      reader.skip(tag & 7);
    }
    return message;
  },

  create(base?: DeepPartial<ResultOutput>): ResultOutput {
    return ResultOutput.fromPartial(base ?? {});
  },
  fromPartial(object: DeepPartial<ResultOutput>): ResultOutput {
    const message = createBaseResultOutput();
    message.items = object.items ?? new Uint8Array(0);
    return message;
  },
};

type Builtin = Date | Function | Uint8Array | string | number | boolean | undefined;

export type DeepPartial<T> = T extends Builtin ? T
  : T extends globalThis.Array<infer U> ? globalThis.Array<DeepPartial<U>>
  : T extends ReadonlyArray<infer U> ? ReadonlyArray<DeepPartial<U>>
  : T extends {} ? { [K in keyof T]?: DeepPartial<T[K]> }
  : Partial<T>;

export interface MessageFns<T> {
  encode(message: T, writer?: BinaryWriter): BinaryWriter;
  decode(input: BinaryReader | Uint8Array, length?: number): T;
  create(base?: DeepPartial<T>): T;
  fromPartial(object: DeepPartial<T>): T;
}
